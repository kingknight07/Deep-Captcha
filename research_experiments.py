#!/usr/bin/env python3
"""
DeepCaptcha Research Experiments
Comprehensive testing suite for evaluating AI resistance effectiveness
against multiple OCR/ML models for research paper publication.

Author: DeepCaptcha Research Team
Date: 2026-02-06
"""

import os
import sys
import json
import time
import numpy as np
from PIL import Image
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Fix Windows console encoding
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except:
        pass
    # Set environment variable for child processes
    os.environ['PYTHONIOENCODING'] = 'utf-8'

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from DeepCaptcha import DeepCaptcha


# Custom JSON encoder to handle numpy types
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)


# =============================================================================
# DATA STRUCTURES
# =============================================================================

@dataclass
class ExperimentConfig:
    """Configuration for experiments."""
    dataset_size_per_level: int = 200  # Images per resistance level
    ai_resistance_levels: List[int] = None
    text_lengths: List[int] = None
    random_seed: int = 42
    output_dir: str = "research_results"
    
    def __post_init__(self):
        if self.ai_resistance_levels is None:
            self.ai_resistance_levels = [0, 1, 2, 3]
        if self.text_lengths is None:
            self.text_lengths = [4, 5, 6]


@dataclass
class ModelResult:
    """Result from a single model prediction."""
    model_name: str
    predicted_text: str
    ground_truth: str
    is_correct: bool
    character_accuracy: float
    confidence: Optional[float] = None
    inference_time_ms: float = 0.0


@dataclass
class ExperimentResult:
    """Results from testing a single image."""
    image_path: str
    ground_truth: str
    ai_resistance_level: int
    text_length: int
    model_results: List[ModelResult] = None
    
    def __post_init__(self):
        if self.model_results is None:
            self.model_results = []


# =============================================================================
# OCR MODEL WRAPPERS
# =============================================================================

class OCRModelBase:
    """Base class for OCR models."""
    def __init__(self, name: str):
        self.name = name
        self.is_available = False
        
    def initialize(self) -> bool:
        """Initialize the model. Returns True if successful."""
        raise NotImplementedError
        
    def predict(self, image: Image.Image) -> Tuple[str, Optional[float]]:
        """Predict text from image. Returns (text, confidence)."""
        raise NotImplementedError


class TesseractOCR(OCRModelBase):
    """Tesseract OCR wrapper."""
    def __init__(self):
        super().__init__("Tesseract")
        self.engine = None
        
    def initialize(self) -> bool:
        try:
            import pytesseract
            # Configure tesseract path for Windows
            tesseract_paths = [
                r'C:\Program Files\Tesseract-OCR\tesseract.exe',
                r'C:\Program Files (x86)\Tesseract-OCR\tesseract.exe',
            ]
            for path in tesseract_paths:
                if os.path.exists(path):
                    pytesseract.pytesseract.tesseract_cmd = path
                    break
            # Test if tesseract is installed
            pytesseract.get_tesseract_version()
            self.engine = pytesseract
            self.is_available = True
            print(f"   ‚úÖ {self.name} initialized successfully")
            return True
        except Exception as e:
            print(f"   ‚ùå {self.name} not available: {e}")
            return False
            
    def predict(self, image: Image.Image) -> Tuple[str, Optional[float]]:
        if not self.is_available:
            return "", None
        try:
            # Configure for CAPTCHA-like text
            config = '--psm 8 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
            text = self.engine.image_to_string(image, config=config)
            text = ''.join(c for c in text.upper() if c.isalnum())
            return text, None
        except:
            return "", None


class EasyOCR_Model(OCRModelBase):
    """EasyOCR wrapper."""
    def __init__(self):
        super().__init__("EasyOCR")
        self.reader = None
        
    def initialize(self) -> bool:
        try:
            import easyocr
            self.reader = easyocr.Reader(['en'], gpu=False, verbose=False)
            self.is_available = True
            print(f"   ‚úÖ {self.name} initialized successfully")
            return True
        except Exception as e:
            print(f"   ‚ùå {self.name} not available: {e}")
            return False
            
    def predict(self, image: Image.Image) -> Tuple[str, Optional[float]]:
        if not self.is_available:
            return "", None
        try:
            result = self.reader.readtext(np.array(image), detail=1)
            if result:
                text = ''.join(r[1] for r in result)
                text = ''.join(c for c in text.upper() if c.isalnum())
                confidence = np.mean([r[2] for r in result]) if result else 0.0
                return text, confidence
            return "", 0.0
        except:
            return "", None


class PaddleOCR_Model(OCRModelBase):
    """PaddleOCR wrapper."""
    def __init__(self):
        super().__init__("PaddleOCR")
        self.ocr = None
        
    def initialize(self) -> bool:
        try:
            from paddleocr import PaddleOCR
            self.ocr = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)
            self.is_available = True
            print(f"   ‚úÖ {self.name} initialized successfully")
            return True
        except Exception as e:
            print(f"   ‚ùå {self.name} not available: {e}")
            return False
            
    def predict(self, image: Image.Image) -> Tuple[str, Optional[float]]:
        if not self.is_available:
            return "", None
        try:
            result = self.ocr.ocr(np.array(image), cls=True)
            if result and result[0]:
                texts = []
                confidences = []
                for line in result[0]:
                    texts.append(line[1][0])
                    confidences.append(line[1][1])
                text = ''.join(texts)
                text = ''.join(c for c in text.upper() if c.isalnum())
                return text, np.mean(confidences) if confidences else 0.0
            return "", 0.0
        except:
            return "", None


class TrOCR_Model(OCRModelBase):
    """TrOCR (Transformer OCR) wrapper from HuggingFace."""
    def __init__(self):
        super().__init__("TrOCR")
        self.processor = None
        self.model = None
        
    def initialize(self) -> bool:
        try:
            from transformers import TrOCRProcessor, VisionEncoderDecoderModel
            self.processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')
            self.model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')
            self.is_available = True
            print(f"   ‚úÖ {self.name} initialized successfully")
            return True
        except Exception as e:
            print(f"   ‚ùå {self.name} not available: {e}")
            return False
            
    def predict(self, image: Image.Image) -> Tuple[str, Optional[float]]:
        if not self.is_available:
            return "", None
        try:
            import torch
            pixel_values = self.processor(images=image.convert("RGB"), return_tensors="pt").pixel_values
            with torch.no_grad():
                generated_ids = self.model.generate(pixel_values)
            text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            text = ''.join(c for c in text.upper() if c.isalnum())
            return text, None
        except:
            return "", None


class KerasOCR_Model(OCRModelBase):
    """Keras-OCR wrapper."""
    def __init__(self):
        super().__init__("KerasOCR")
        self.pipeline = None
        
    def initialize(self) -> bool:
        try:
            import keras_ocr
            self.pipeline = keras_ocr.pipeline.Pipeline()
            self.is_available = True
            print(f"   ‚úÖ {self.name} initialized successfully")
            return True
        except Exception as e:
            print(f"   ‚ùå {self.name} not available: {e}")
            return False
            
    def predict(self, image: Image.Image) -> Tuple[str, Optional[float]]:
        if not self.is_available:
            return "", None
        try:
            img_array = np.array(image.convert('RGB'))
            predictions = self.pipeline.recognize([img_array])
            if predictions and predictions[0]:
                text = ''.join([pred[0] for pred in predictions[0]])
                text = ''.join(c for c in text.upper() if c.isalnum())
                return text, None
            return "", None
        except:
            return "", None


class DocTR_Model(OCRModelBase):
    """DocTR (Document Text Recognition) wrapper from Mindee."""
    def __init__(self):
        super().__init__("DocTR")
        self.model = None
        
    def initialize(self) -> bool:
        try:
            from doctr.io import DocumentFile
            from doctr.models import ocr_predictor
            self.model = ocr_predictor(pretrained=True)
            self.is_available = True
            print(f"   ‚úÖ {self.name} initialized successfully")
            return True
        except Exception as e:
            print(f"   ‚ùå {self.name} not available: {e}")
            return False
            
    def predict(self, image: Image.Image) -> Tuple[str, Optional[float]]:
        if not self.is_available:
            return "", None
        try:
            # Save image temporarily for doctr
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                image.save(tmp.name)
                from doctr.io import DocumentFile
                doc = DocumentFile.from_images(tmp.name)
                result = self.model(doc)
                os.unlink(tmp.name)
                
            # Extract text from result
            texts = []
            for page in result.pages:
                for block in page.blocks:
                    for line in block.lines:
                        for word in line.words:
                            texts.append(word.value)
            text = ''.join(texts)
            text = ''.join(c for c in text.upper() if c.isalnum())
            return text, None
        except:
            return "", None


# =============================================================================
# EXPERIMENT RUNNER
# =============================================================================

class ResearchExperiment:
    """Main experiment runner for research paper."""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.models: List[OCRModelBase] = []
        self.results: List[ExperimentResult] = []
        self.dataset_metadata: List[Dict] = []
        
        # Create output directory
        os.makedirs(config.output_dir, exist_ok=True)
        os.makedirs(os.path.join(config.output_dir, "dataset"), exist_ok=True)
        os.makedirs(os.path.join(config.output_dir, "figures"), exist_ok=True)
        
    def initialize_models(self):
        """Initialize all available OCR models."""
        print("\nüîß Initializing OCR Models...")
        print("-" * 50)
        
        model_classes = [
            TesseractOCR,
            EasyOCR_Model,
            PaddleOCR_Model,
            TrOCR_Model,
            KerasOCR_Model,
            DocTR_Model,
        ]
        
        for model_class in model_classes:
            model = model_class()
            if model.initialize():
                self.models.append(model)
                
        print(f"\nüìä {len(self.models)} models available for testing")
        
        if len(self.models) == 0:
            print("\n‚ö†Ô∏è  No OCR models available! Installing recommended models...")
            print("   Run: pip install pytesseract easyocr")
            
        return len(self.models) > 0
    
    def generate_research_dataset(self) -> str:
        """Generate a research-quality dataset."""
        print("\nüìÅ Generating Research Dataset...")
        print("-" * 50)
        
        np.random.seed(self.config.random_seed)
        
        dataset_dir = os.path.join(self.config.output_dir, "dataset")
        total_images = self.config.dataset_size_per_level * len(self.config.ai_resistance_levels)
        
        print(f"   Total images to generate: {total_images}")
        print(f"   AI resistance levels: {self.config.ai_resistance_levels}")
        print(f"   Text lengths: {self.config.text_lengths}")
        
        image_count = 0
        start_time = time.time()
        
        for ai_level in self.config.ai_resistance_levels:
            level_dir = os.path.join(dataset_dir, f"level_{ai_level}")
            os.makedirs(level_dir, exist_ok=True)
            
            print(f"\n   üîÑ Generating Level {ai_level} images...")
            
            for i in range(self.config.dataset_size_per_level):
                # Randomize parameters for diversity
                text_length = np.random.choice(self.config.text_lengths)
                
                captcha = DeepCaptcha(
                    width=280,
                    height=100,
                    text_length=text_length,
                    num_lines=np.random.randint(4, 8),
                    line_thickness=np.random.randint(2, 4),
                    blur_level=np.random.uniform(0.2, 0.5),
                    noise_density=np.random.uniform(0.2, 0.4),
                    color_mode=True,
                    ai_resistance_level=ai_level
                )
                
                image, text = captcha.generate()
                
                # Save image
                filename = f"level{ai_level}_{i:04d}_{text}.png"
                filepath = os.path.join(level_dir, filename)
                image.save(filepath)
                
                # Store metadata
                self.dataset_metadata.append({
                    'filepath': filepath,
                    'filename': filename,
                    'ground_truth': text,
                    'ai_resistance_level': ai_level,
                    'text_length': text_length,
                    'index': image_count
                })
                
                image_count += 1
                
                if (i + 1) % 50 == 0:
                    print(f"      Progress: {i+1}/{self.config.dataset_size_per_level}")
        
        elapsed = time.time() - start_time
        print(f"\n   ‚úÖ Generated {image_count} images in {elapsed:.1f}s")
        print(f"   üìà Speed: {image_count/elapsed:.1f} images/second")
        
        # Save metadata
        metadata_path = os.path.join(self.config.output_dir, "dataset_metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.dataset_metadata, f, indent=2, cls=NumpyEncoder)
        print(f"   üíæ Metadata saved to: {metadata_path}")
        
        return dataset_dir
    
    def run_experiments(self):
        """Run all experiments on the dataset."""
        print("\nüß™ Running OCR Experiments...")
        print("-" * 50)
        
        if not self.models:
            print("‚ùå No models available for testing!")
            return
            
        total_images = len(self.dataset_metadata)
        print(f"   Total images: {total_images}")
        print(f"   Models: {[m.name for m in self.models]}")
        
        start_time = time.time()
        
        for idx, meta in enumerate(self.dataset_metadata):
            # Load image
            image = Image.open(meta['filepath'])
            
            # Create experiment result
            exp_result = ExperimentResult(
                image_path=meta['filepath'],
                ground_truth=meta['ground_truth'],
                ai_resistance_level=meta['ai_resistance_level'],
                text_length=meta['text_length']
            )
            
            # Test each model
            for model in self.models:
                model_start = time.time()
                predicted, confidence = model.predict(image)
                inference_time = (time.time() - model_start) * 1000
                
                # Calculate accuracy
                is_correct = predicted == meta['ground_truth']
                char_accuracy = self._calculate_char_accuracy(
                    predicted, meta['ground_truth']
                )
                
                model_result = ModelResult(
                    model_name=model.name,
                    predicted_text=predicted,
                    ground_truth=meta['ground_truth'],
                    is_correct=is_correct,
                    character_accuracy=char_accuracy,
                    confidence=confidence,
                    inference_time_ms=inference_time
                )
                exp_result.model_results.append(model_result)
            
            self.results.append(exp_result)
            
            # Progress
            if (idx + 1) % 50 == 0 or idx == 0:
                elapsed = time.time() - start_time
                eta = (elapsed / (idx + 1)) * (total_images - idx - 1)
                print(f"   Progress: {idx+1}/{total_images} ({(idx+1)/total_images*100:.1f}%) - ETA: {eta:.0f}s")
        
        total_time = time.time() - start_time
        print(f"\n   ‚úÖ Experiments completed in {total_time:.1f}s")
        print(f"   üìà Average: {total_time/total_images*1000:.1f}ms per image")
    
    def _calculate_char_accuracy(self, predicted: str, ground_truth: str) -> float:
        """Calculate character-level accuracy."""
        if not ground_truth:
            return 0.0
        if not predicted:
            return 0.0
            
        # Pad shorter string
        max_len = max(len(predicted), len(ground_truth))
        predicted = predicted.ljust(max_len)
        ground_truth = ground_truth.ljust(max_len)
        
        correct = sum(p == g for p, g in zip(predicted, ground_truth))
        return correct / len(ground_truth)
    
    def analyze_results(self) -> Dict:
        """Analyze experiment results and generate statistics."""
        print("\nüìä Analyzing Results...")
        print("-" * 50)
        
        analysis = {
            'experiment_info': {
                'date': datetime.now().isoformat(),
                'total_images': len(self.results),
                'ai_resistance_levels': self.config.ai_resistance_levels,
                'models_tested': [m.name for m in self.models],
                'images_per_level': self.config.dataset_size_per_level
            },
            'overall_results': {},
            'per_level_results': {},
            'per_model_results': {},
            'statistical_analysis': {}
        }
        
        # Organize results by model and level
        for model in self.models:
            analysis['overall_results'][model.name] = {
                'total_correct': 0,
                'total_char_accuracy': 0.0,
                'total_images': 0,
                'avg_inference_time_ms': 0.0
            }
            analysis['per_model_results'][model.name] = {}
            
            for level in self.config.ai_resistance_levels:
                analysis['per_model_results'][model.name][f'level_{level}'] = {
                    'correct': 0,
                    'total': 0,
                    'char_accuracies': [],
                    'inference_times': []
                }
        
        # Process results
        for result in self.results:
            level = result.ai_resistance_level
            
            for model_result in result.model_results:
                model_name = model_result.model_name
                level_key = f'level_{level}'
                
                # Update per-level stats
                level_stats = analysis['per_model_results'][model_name][level_key]
                level_stats['total'] += 1
                if model_result.is_correct:
                    level_stats['correct'] += 1
                level_stats['char_accuracies'].append(model_result.character_accuracy)
                level_stats['inference_times'].append(model_result.inference_time_ms)
                
                # Update overall stats
                overall = analysis['overall_results'][model_name]
                overall['total_images'] += 1
                if model_result.is_correct:
                    overall['total_correct'] += 1
                overall['total_char_accuracy'] += model_result.character_accuracy
        
        # Calculate final statistics
        for model in self.models:
            model_name = model.name
            overall = analysis['overall_results'][model_name]
            
            if overall['total_images'] > 0:
                overall['exact_match_accuracy'] = overall['total_correct'] / overall['total_images'] * 100
                overall['avg_char_accuracy'] = overall['total_char_accuracy'] / overall['total_images'] * 100
            
            # Per-level statistics
            for level in self.config.ai_resistance_levels:
                level_key = f'level_{level}'
                level_stats = analysis['per_model_results'][model_name][level_key]
                
                if level_stats['total'] > 0:
                    level_stats['exact_match_accuracy'] = level_stats['correct'] / level_stats['total'] * 100
                    level_stats['avg_char_accuracy'] = np.mean(level_stats['char_accuracies']) * 100
                    level_stats['std_char_accuracy'] = np.std(level_stats['char_accuracies']) * 100
                    level_stats['avg_inference_time'] = np.mean(level_stats['inference_times'])
                    
                    # Remove raw data to save space
                    del level_stats['char_accuracies']
                    del level_stats['inference_times']
        
        # Calculate relative accuracy drop
        print("\nüìâ AI Resistance Effectiveness:")
        print("-" * 60)
        
        analysis['ai_resistance_effectiveness'] = {}
        
        for model in self.models:
            model_name = model.name
            baseline = analysis['per_model_results'][model_name]['level_0']
            
            analysis['ai_resistance_effectiveness'][model_name] = {}
            
            print(f"\n{model_name}:")
            print(f"   Baseline (Level 0): {baseline.get('exact_match_accuracy', 0):.1f}% exact match")
            
            for level in [1, 2, 3]:
                level_key = f'level_{level}'
                level_stats = analysis['per_model_results'][model_name][level_key]
                
                baseline_acc = baseline.get('exact_match_accuracy', 0)
                level_acc = level_stats.get('exact_match_accuracy', 0)
                
                if baseline_acc > 0:
                    accuracy_drop = ((baseline_acc - level_acc) / baseline_acc) * 100
                else:
                    accuracy_drop = 0
                    
                analysis['ai_resistance_effectiveness'][model_name][level_key] = {
                    'accuracy': level_acc,
                    'accuracy_drop_percent': accuracy_drop
                }
                
                print(f"   Level {level}: {level_acc:.1f}% exact match (‚Üì{accuracy_drop:.1f}% from baseline)")
        
        return analysis
    
    def generate_research_figures(self, analysis: Dict):
        """Generate publication-quality figures."""
        print("\nüìà Generating Research Figures...")
        print("-" * 50)
        
        try:
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')
            plt.style.use('seaborn-v0_8-whitegrid')
        except:
            try:
                import matplotlib.pyplot as plt
                plt.style.use('ggplot')
            except:
                print("   ‚ö†Ô∏è Matplotlib not available, skipping figures")
                return
        
        figures_dir = os.path.join(self.config.output_dir, "figures")
        
        # Figure 1: Accuracy vs AI Resistance Level
        self._plot_accuracy_vs_resistance(analysis, figures_dir)
        
        # Figure 2: Model Comparison Bar Chart
        self._plot_model_comparison(analysis, figures_dir)
        
        # Figure 3: Accuracy Drop Heatmap
        self._plot_accuracy_drop_heatmap(analysis, figures_dir)
        
        # Figure 4: Character Accuracy Distribution
        self._plot_char_accuracy_distribution(analysis, figures_dir)
        
        print(f"   ‚úÖ Figures saved to: {figures_dir}")
    
    def _plot_accuracy_vs_resistance(self, analysis: Dict, output_dir: str):
        """Plot accuracy vs AI resistance level."""
        import matplotlib.pyplot as plt
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        levels = [0, 1, 2, 3]
        colors = plt.cm.tab10(np.linspace(0, 1, len(self.models)))
        
        for i, model in enumerate(self.models):
            accuracies = []
            for level in levels:
                level_key = f'level_{level}'
                acc = analysis['per_model_results'][model.name][level_key].get('exact_match_accuracy', 0)
                accuracies.append(acc)
            
            ax.plot(levels, accuracies, 'o-', label=model.name, 
                   linewidth=2, markersize=8, color=colors[i])
        
        ax.set_xlabel('AI Resistance Level', fontsize=12, fontweight='bold')
        ax.set_ylabel('Exact Match Accuracy (%)', fontsize=12, fontweight='bold')
        ax.set_title('OCR Model Accuracy vs DeepCaptcha AI Resistance Level', 
                    fontsize=14, fontweight='bold')
        ax.set_xticks(levels)
        ax.set_xticklabels(['0\n(Baseline)', '1\n(Basic)', '2\n(Moderate)', '3\n(Advanced)'])
        ax.legend(loc='upper right')
        ax.set_ylim(0, 100)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'accuracy_vs_resistance.png'), dpi=300)
        plt.savefig(os.path.join(output_dir, 'accuracy_vs_resistance.pdf'))
        plt.close()
        print("   üìä Created: accuracy_vs_resistance.png/pdf")
    
    def _plot_model_comparison(self, analysis: Dict, output_dir: str):
        """Plot model comparison bar chart."""
        import matplotlib.pyplot as plt
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        models = [m.name for m in self.models]
        x = np.arange(len(models))
        width = 0.2
        
        colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']
        labels = ['Level 0 (Baseline)', 'Level 1 (Basic)', 'Level 2 (Moderate)', 'Level 3 (Advanced)']
        
        for i, level in enumerate([0, 1, 2, 3]):
            accuracies = []
            for model in models:
                level_key = f'level_{level}'
                acc = analysis['per_model_results'][model][level_key].get('exact_match_accuracy', 0)
                accuracies.append(acc)
            
            bars = ax.bar(x + i * width, accuracies, width, label=labels[i], color=colors[i])
            
            # Add value labels
            for bar, acc in zip(bars, accuracies):
                if acc > 0:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                           f'{acc:.0f}%', ha='center', va='bottom', fontsize=8, rotation=90)
        
        ax.set_xlabel('OCR Model', fontsize=12, fontweight='bold')
        ax.set_ylabel('Exact Match Accuracy (%)', fontsize=12, fontweight='bold')
        ax.set_title('Model Comparison Across AI Resistance Levels', fontsize=14, fontweight='bold')
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend(loc='upper right')
        ax.set_ylim(0, 110)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'model_comparison.png'), dpi=300)
        plt.savefig(os.path.join(output_dir, 'model_comparison.pdf'))
        plt.close()
        print("   üìä Created: model_comparison.png/pdf")
    
    def _plot_accuracy_drop_heatmap(self, analysis: Dict, output_dir: str):
        """Plot accuracy drop heatmap."""
        import matplotlib.pyplot as plt
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        models = [m.name for m in self.models]
        levels = [1, 2, 3]
        
        data = []
        for model in models:
            row = []
            for level in levels:
                level_key = f'level_{level}'
                drop = analysis['ai_resistance_effectiveness'].get(model, {}).get(level_key, {}).get('accuracy_drop_percent', 0)
                row.append(drop)
            data.append(row)
        
        data = np.array(data)
        
        im = ax.imshow(data, cmap='RdYlGn_r', aspect='auto', vmin=0, vmax=100)
        
        ax.set_xticks(np.arange(len(levels)))
        ax.set_yticks(np.arange(len(models)))
        ax.set_xticklabels(['Level 1\n(Basic)', 'Level 2\n(Moderate)', 'Level 3\n(Advanced)'])
        ax.set_yticklabels(models)
        
        # Add text annotations
        for i in range(len(models)):
            for j in range(len(levels)):
                text = ax.text(j, i, f'{data[i, j]:.1f}%',
                              ha='center', va='center', color='black', fontweight='bold')
        
        ax.set_title('Accuracy Drop from Baseline per AI Resistance Level', 
                    fontsize=14, fontweight='bold')
        ax.set_xlabel('AI Resistance Level', fontsize=12, fontweight='bold')
        ax.set_ylabel('OCR Model', fontsize=12, fontweight='bold')
        
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('Accuracy Drop (%)', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'accuracy_drop_heatmap.png'), dpi=300)
        plt.savefig(os.path.join(output_dir, 'accuracy_drop_heatmap.pdf'))
        plt.close()
        print("   üìä Created: accuracy_drop_heatmap.png/pdf")
    
    def _plot_char_accuracy_distribution(self, analysis: Dict, output_dir: str):
        """Plot character accuracy distribution."""
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        for idx, level in enumerate([0, 1, 2, 3]):
            ax = axes[idx]
            
            for model in self.models:
                level_key = f'level_{level}'
                avg_acc = analysis['per_model_results'][model.name][level_key].get('avg_char_accuracy', 0)
                std_acc = analysis['per_model_results'][model.name][level_key].get('std_char_accuracy', 0)
                
                ax.bar(model.name, avg_acc, yerr=std_acc, capsize=5, alpha=0.7)
            
            ax.set_title(f'Level {level}', fontsize=12, fontweight='bold')
            ax.set_ylabel('Character Accuracy (%)')
            ax.set_ylim(0, 100)
            ax.tick_params(axis='x', rotation=45)
        
        fig.suptitle('Character-Level Accuracy Distribution', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'char_accuracy_distribution.png'), dpi=300)
        plt.savefig(os.path.join(output_dir, 'char_accuracy_distribution.pdf'))
        plt.close()
        print("   üìä Created: char_accuracy_distribution.png/pdf")
    
    def save_results(self, analysis: Dict):
        """Save all results for research paper."""
        print("\nüíæ Saving Research Results...")
        print("-" * 50)
        
        # Save detailed analysis
        analysis_path = os.path.join(self.config.output_dir, "research_analysis.json")
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, cls=NumpyEncoder)
        print(f"   üìã Analysis saved to: {analysis_path}")
        
        # Save detailed results
        results_data = []
        for result in self.results:
            result_dict = {
                'image_path': result.image_path,
                'ground_truth': result.ground_truth,
                'ai_resistance_level': result.ai_resistance_level,
                'text_length': result.text_length,
                'model_results': [asdict(mr) for mr in result.model_results]
            }
            results_data.append(result_dict)
        
        results_path = os.path.join(self.config.output_dir, "detailed_results.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, cls=NumpyEncoder)
        print(f"   üìã Detailed results saved to: {results_path}")
        
        # Generate LaTeX table for paper
        self._generate_latex_tables(analysis)
        
        # Generate markdown summary
        self._generate_markdown_summary(analysis)
    
    def _generate_latex_tables(self, analysis: Dict):
        """Generate LaTeX tables for research paper."""
        latex_path = os.path.join(self.config.output_dir, "latex_tables.tex")
        
        with open(latex_path, 'w', encoding='utf-8') as f:
            f.write("% DeepCaptcha Research Results - LaTeX Tables\n")
            f.write("% Generated: " + datetime.now().isoformat() + "\n\n")
            
            # Table 1: Overall Results
            f.write("% Table 1: Overall Accuracy Results\n")
            f.write("\\begin{table}[h]\n")
            f.write("\\centering\n")
            f.write("\\caption{OCR Model Accuracy vs AI Resistance Level}\n")
            f.write("\\label{tab:accuracy}\n")
            f.write("\\begin{tabular}{l|cccc}\n")
            f.write("\\hline\n")
            f.write("\\textbf{Model} & \\textbf{Level 0} & \\textbf{Level 1} & \\textbf{Level 2} & \\textbf{Level 3} \\\\\n")
            f.write("\\hline\n")
            
            for model in self.models:
                row = [model.name]
                for level in [0, 1, 2, 3]:
                    level_key = f'level_{level}'
                    acc = analysis['per_model_results'][model.name][level_key].get('exact_match_accuracy', 0)
                    row.append(f"{acc:.1f}\\%")
                f.write(" & ".join(row) + " \\\\\n")
            
            f.write("\\hline\n")
            f.write("\\end{tabular}\n")
            f.write("\\end{table}\n\n")
            
            # Table 2: Accuracy Drop
            f.write("% Table 2: Accuracy Drop from Baseline\n")
            f.write("\\begin{table}[h]\n")
            f.write("\\centering\n")
            f.write("\\caption{Accuracy Drop from Baseline (Level 0)}\n")
            f.write("\\label{tab:drop}\n")
            f.write("\\begin{tabular}{l|ccc}\n")
            f.write("\\hline\n")
            f.write("\\textbf{Model} & \\textbf{Level 1} & \\textbf{Level 2} & \\textbf{Level 3} \\\\\n")
            f.write("\\hline\n")
            
            for model in self.models:
                row = [model.name]
                for level in [1, 2, 3]:
                    level_key = f'level_{level}'
                    drop = analysis['ai_resistance_effectiveness'].get(model.name, {}).get(level_key, {}).get('accuracy_drop_percent', 0)
                    row.append(f"{drop:.1f}\\%")
                f.write(" & ".join(row) + " \\\\\n")
            
            f.write("\\hline\n")
            f.write("\\end{tabular}\n")
            f.write("\\end{table}\n")
        
        print(f"   üìÑ LaTeX tables saved to: {latex_path}")
    
    def _generate_markdown_summary(self, analysis: Dict):
        """Generate markdown summary for GitHub/documentation."""
        md_path = os.path.join(self.config.output_dir, "RESEARCH_SUMMARY.md")
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write("# DeepCaptcha AI Resistance Research Results\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## Experiment Configuration\n\n")
            f.write(f"- **Total Images:** {analysis['experiment_info']['total_images']}\n")
            f.write(f"- **Images per Level:** {analysis['experiment_info']['images_per_level']}\n")
            f.write(f"- **Models Tested:** {', '.join(analysis['experiment_info']['models_tested'])}\n")
            f.write(f"- **AI Resistance Levels:** {analysis['experiment_info']['ai_resistance_levels']}\n\n")
            
            f.write("## Results Summary\n\n")
            f.write("### Exact Match Accuracy (%)\n\n")
            f.write("| Model | Level 0 | Level 1 | Level 2 | Level 3 |\n")
            f.write("|-------|---------|---------|---------|----------|\n")
            
            for model in self.models:
                row = [model.name]
                for level in [0, 1, 2, 3]:
                    level_key = f'level_{level}'
                    acc = analysis['per_model_results'][model.name][level_key].get('exact_match_accuracy', 0)
                    row.append(f"{acc:.1f}%")
                f.write("| " + " | ".join(row) + " |\n")
            
            f.write("\n### Accuracy Drop from Baseline\n\n")
            f.write("| Model | Level 1 | Level 2 | Level 3 |\n")
            f.write("|-------|---------|---------|----------|\n")
            
            for model in self.models:
                row = [model.name]
                for level in [1, 2, 3]:
                    level_key = f'level_{level}'
                    drop = analysis['ai_resistance_effectiveness'].get(model.name, {}).get(level_key, {}).get('accuracy_drop_percent', 0)
                    row.append(f"‚Üì{drop:.1f}%")
                f.write("| " + " | ".join(row) + " |\n")
            
            f.write("\n## Key Findings\n\n")
            
            # Calculate average drops
            avg_drops = {1: [], 2: [], 3: []}
            for model in self.models:
                for level in [1, 2, 3]:
                    level_key = f'level_{level}'
                    drop = analysis['ai_resistance_effectiveness'].get(model.name, {}).get(level_key, {}).get('accuracy_drop_percent', 0)
                    avg_drops[level].append(drop)
            
            for level in [1, 2, 3]:
                avg = np.mean(avg_drops[level]) if avg_drops[level] else 0
                f.write(f"- **Level {level}:** Average accuracy drop of {avg:.1f}% across all models\n")
            
            f.write("\n## Conclusion\n\n")
            f.write("DeepCaptcha's AI resistance technology demonstrates measurable effectiveness ")
            f.write("in reducing OCR model accuracy while maintaining human readability.\n")
        
        print(f"   üìù Summary saved to: {md_path}")


# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Main entry point for research experiments."""
    print("=" * 70)
    print("üî¨ DeepCaptcha Research Experiments")
    print("   Evaluating AI Resistance Against OCR/ML Models")
    print("=" * 70)
    
    # Configure experiment
    config = ExperimentConfig(
        dataset_size_per_level=200,  # 200 images per level = 800 total
        ai_resistance_levels=[0, 1, 2, 3],
        text_lengths=[4, 5, 6],
        random_seed=42,
        output_dir="research_results"
    )
    
    # Create experiment runner
    experiment = ResearchExperiment(config)
    
    # Initialize models
    if not experiment.initialize_models():
        print("\n‚ö†Ô∏è  Warning: No OCR models available!")
        print("   Please install at least one: pip install pytesseract easyocr")
        print("   Continuing with dataset generation only...\n")
    
    # Generate dataset
    experiment.generate_research_dataset()
    
    # Run experiments (if models available)
    if experiment.models:
        experiment.run_experiments()
        
        # Analyze results
        analysis = experiment.analyze_results()
        
        # Generate figures
        experiment.generate_research_figures(analysis)
        
        # Save results
        experiment.save_results(analysis)
    else:
        print("\n‚ö†Ô∏è  Skipping experiments - no OCR models available")
        print("   Dataset generated successfully for manual testing")
    
    print("\n" + "=" * 70)
    print("‚úÖ Research Experiments Complete!")
    print("=" * 70)
    print(f"\nüìÅ Results saved to: {os.path.abspath(config.output_dir)}/")
    print("   - research_analysis.json: Complete analysis data")
    print("   - detailed_results.json: Per-image results")
    print("   - latex_tables.tex: Ready for paper")
    print("   - RESEARCH_SUMMARY.md: GitHub-ready summary")
    print("   - figures/: Publication-quality graphs")


if __name__ == "__main__":
    main()
